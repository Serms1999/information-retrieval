{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Los he ordenado por similaritud, me he estado liando un poco con lo de los excells, os dejo los dos formatos, no se cual os gustara mas, pero vaya qye si cinseguis juntarlos bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me ha hecho falta instalar esto\n",
    "pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## este es el primero, que deja la query oero el nombre de las columnas no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   loinc_num                                   long_common_name  \\\n",
      "0     1988-5  C reactive protein [Mass/volume] in Serum or P...   \n",
      "43   30522-7  C reactive protein [Mass/volume] in Serum or P...   \n",
      "10    4671-4                  Protein C [Mass/volume] in Plasma   \n",
      "8     2143-6          Cortisol [Mass/volume] in Serum or Plasma   \n",
      "21    1968-7  Bilirubin.direct [Mass/volume] in Serum or Plasma   \n",
      "..       ...                                                ...   \n",
      "11   18864-9                        Ampicillin [Susceptibility]   \n",
      "24    8310-5                                   Body temperature   \n",
      "17     925-8                   Blood product disposition [Type]   \n",
      "48   18955-5                    Nitrofurantoin [Susceptibility]   \n",
      "66   23658-8                  Other Antibiotic [Susceptibility]   \n",
      "\n",
      "                                           component    system  Similarity  \n",
      "0                                 C reactive protein  Ser/Plas    1.000000  \n",
      "43                                C reactive protein  Ser/Plas    0.970143  \n",
      "10                                         Protein C      Plas    0.727607  \n",
      "8                                           Cortisol  Ser/Plas    0.685994  \n",
      "21  Bilirubin.glucuronidated+Bilirubin.albumin bound  Ser/Plas    0.685994  \n",
      "..                                               ...       ...         ...  \n",
      "11                                        Ampicillin   Isolate    0.000000  \n",
      "24                                  Body temperature  ^Patient    0.000000  \n",
      "17                         Blood product disposition      ^BPU    0.000000  \n",
      "48                                    Nitrofurantoin   Isolate    0.000000  \n",
      "66                                    Antibiotic XXX   Isolate    0.000000  \n",
      "\n",
      "[67 rows x 5 columns]\n",
      "The ranked DataFrame has been saved to data/ranked_loinc_dataset_same_format.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Read data from Excel file into a dictionary of DataFrames (one for each sheet)\n",
    "file_path = 'data/loinc_dataset-v2.xlsx'\n",
    "excel_data = pd.read_excel(file_path, sheet_name=None, skiprows=2)\n",
    "\n",
    "# Create a new Excel file to store the ranked results\n",
    "output_file_path = 'data/ranked_loinc_dataset_same_format_multi_sheets.xlsx'\n",
    "with pd.ExcelWriter(output_file_path, engine='xlsxwriter') as writer:\n",
    "    # Iterate through each sheet\n",
    "    for sheet_name, df in excel_data.items():\n",
    "        # Drop empty rows\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Extract features for the query row\n",
    "        query_row = df.iloc[0]  # Assuming the first row is the query\n",
    "        query_features = query_row[['loinc_num', 'long_common_name', 'component', 'system']].astype(str)\n",
    "\n",
    "        # Combine the text columns into a single string for vectorization\n",
    "        query_text = ' '.join(query_features.values)\n",
    "\n",
    "        # Vectorize the text using TF-IDF for the query\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        query_vectorized = vectorizer.fit_transform([query_text])\n",
    "\n",
    "        # Initialize a list to store similarity scores\n",
    "        similarity_scores = []\n",
    "\n",
    "        # Iterate through each data row until the next empty row and calculate similarity\n",
    "        for index, row in df.iterrows():\n",
    "            # Check for an empty row\n",
    "            if row.isnull().all():\n",
    "                break\n",
    "\n",
    "            data_features = row[['loinc_num', 'long_common_name', 'component', 'system']].astype(str)\n",
    "            data_text = ' '.join(data_features.values)\n",
    "\n",
    "            # Vectorize the text using TF-IDF for the data row\n",
    "            data_vectorized = vectorizer.transform([data_text])\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            similarity_score = cosine_similarity(query_vectorized, data_vectorized)[0][0]\n",
    "\n",
    "            # Append similarity score to the list\n",
    "            similarity_scores.append(similarity_score)\n",
    "\n",
    "        # Add similarity scores to the DataFrame\n",
    "        df['Similarity'] = similarity_scores\n",
    "\n",
    "        # Sort DataFrame by similarity in descending order\n",
    "        ranked_df = df.sort_values(by='Similarity', ascending=False)\n",
    "\n",
    "        # Create a new DataFrame with the required format\n",
    "        result_df = pd.DataFrame(columns=ranked_df.columns)\n",
    "        result_df.loc[0] = [f'Query: {sheet_name}'] + [''] * (result_df.shape[1] - 1)\n",
    "        result_df.loc[1] = [''] * result_df.shape[1]\n",
    "\n",
    "        # Concatenate the new DataFrame with the column names and the ranked DataFrame\n",
    "        result_df = pd.concat([result_df, ranked_df], ignore_index=True)\n",
    "\n",
    "        # Save the result DataFrame to the Excel file with the original sheet name\n",
    "        result_df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)\n",
    "\n",
    "# Print a message indicating that the DataFrame has been saved\n",
    "print(f\"The final ranked DataFrame has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y este es el segundo, que deja las columnas pero no el nombre de la query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Read data from Excel file into a dictionary of DataFrames (one for each sheet)\n",
    "file_path = 'data/loinc_dataset-v2.xlsx'\n",
    "excel_data = pd.read_excel(file_path, sheet_name=None, skiprows=2)\n",
    "\n",
    "# Create a new Excel file to store the ranked results\n",
    "output_file_path = 'data/ranked_loinc_dataset_same_format_multi_sheets.xlsx'\n",
    "with pd.ExcelWriter(output_file_path, engine='xlsxwriter') as writer:\n",
    "    # Iterate through each sheet\n",
    "    for sheet_name, df in excel_data.items():\n",
    "        # Drop empty rows\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Extract features for the query row\n",
    "        query_row = df.iloc[0]  # Assuming the first row is the query\n",
    "        query_features = query_row[['loinc_num', 'long_common_name', 'component', 'system']].astype(str)\n",
    "\n",
    "        # Combine the text columns into a single string for vectorization\n",
    "        query_text = ' '.join(query_features.values)\n",
    "\n",
    "        # Vectorize the text using TF-IDF for the query\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        query_vectorized = vectorizer.fit_transform([query_text])\n",
    "\n",
    "        # Initialize a list to store similarity scores\n",
    "        similarity_scores = []\n",
    "\n",
    "        # Iterate through each data row until the next empty row and calculate similarity\n",
    "        for index, row in df.iterrows():\n",
    "            # Check for an empty row\n",
    "            if row.isnull().all():\n",
    "                break\n",
    "\n",
    "            data_features = row[['loinc_num', 'long_common_name', 'component', 'system']].astype(str)\n",
    "            data_text = ' '.join(data_features.values)\n",
    "\n",
    "            # Vectorize the text using TF-IDF for the data row\n",
    "            data_vectorized = vectorizer.transform([data_text])\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            similarity_score = cosine_similarity(query_vectorized, data_vectorized)[0][0]\n",
    "\n",
    "            # Append similarity score to the list\n",
    "            similarity_scores.append(similarity_score)\n",
    "\n",
    "        # Add similarity scores to the DataFrame\n",
    "        df['Similarity'] = similarity_scores\n",
    "\n",
    "        # Sort DataFrame by similarity in descending order\n",
    "        ranked_df = df.sort_values(by='Similarity', ascending=False)\n",
    "\n",
    "        # Create a new DataFrame with the required format\n",
    "        result_df = pd.DataFrame(columns=ranked_df.columns)\n",
    "        result_df.loc[0] = [None] * (result_df.shape[1])  # Initialize with None\n",
    "        result_df.loc[0, 'loinc_num'] = 'loinc_num'\n",
    "        result_df.loc[0, 'long_common_name'] = 'long_common_name'\n",
    "        result_df.loc[0, 'component'] = 'component'\n",
    "        result_df.loc[0, 'system'] = 'system'\n",
    "        result_df.loc[0, 'Similarity'] = 'Similarity'\n",
    "\n",
    "        # Concatenate the new DataFrame with the column names and the ranked DataFrame\n",
    "        result_df = pd.concat([result_df, ranked_df], ignore_index=True)\n",
    "\n",
    "        # Save the result DataFrame to the Excel file with the original sheet name\n",
    "        result_df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)\n",
    "\n",
    "# Print a message indicating that the DataFrame has been saved\n",
    "print(f\"The final ranked DataFrame has been saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esto no esta bien, es aplicando lo de la regresion pero vaya qye no esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m data_vectorized \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([data_text])\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Use logistic regression to predict rank (binary classification)\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m logistic_score \u001b[38;5;241m=\u001b[39m logistic_model\u001b[38;5;241m.\u001b[39mfit(query_vectorized, [\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mdata_vectorized\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mpredict_proba(data_vectorized)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Append logistic score to the list\u001b[39;00m\n\u001b[0;32m     43\u001b[0m logistic_scores\u001b[38;5;241m.\u001b[39mappend(logistic_score[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\ander\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ander\\Anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1252\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1250\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1256\u001b[0m         \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1257\u001b[0m     )\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1260\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Read data from Excel file into a DataFrame, skipping the empty row and starting from the third row\n",
    "file_path = 'loinc_dataset-v2.xlsx'\n",
    "df = pd.read_excel(file_path, skiprows=2)\n",
    "df = df.dropna()\n",
    "\n",
    "# Extract features for the query row\n",
    "query_row = df.iloc[0]  # Assuming the first row is the query\n",
    "query_features = query_row[['loinc_num', 'long_common_name', 'component', 'system']].astype(str)\n",
    "\n",
    "# Combine the text columns into a single string for vectorization\n",
    "query_text = ' '.join(query_features.values)\n",
    "\n",
    "# Vectorize the text using TF-IDF for the query\n",
    "vectorizer = TfidfVectorizer()\n",
    "query_vectorized = vectorizer.fit_transform([query_text])\n",
    "\n",
    "# Initialize a list to store logistic regression scores\n",
    "logistic_scores = []\n",
    "\n",
    "# Initialize logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Iterate through each data row until the next empty row and apply logistic regression\n",
    "for index, row in df.iterrows():\n",
    "    # Check for an empty row\n",
    "    if row.isnull().all():\n",
    "        break\n",
    "    \n",
    "    data_features = row[['loinc_num', 'long_common_name', 'component', 'system']].astype(str)\n",
    "    data_text = ' '.join(data_features.values)\n",
    "    \n",
    "    # Vectorize the text using TF-IDF for the data row\n",
    "    data_vectorized = vectorizer.transform([data_text])\n",
    "    \n",
    "    # Use logistic regression to predict rank (binary classification)\n",
    "    logistic_score = logistic_model.fit(query_vectorized, [1]*len(data_vectorized)).predict_proba(data_vectorized)[:, 1]\n",
    "    \n",
    "    # Append logistic score to the list\n",
    "    logistic_scores.append(logistic_score[0])\n",
    "\n",
    "# Add logistic scores to the DataFrame\n",
    "df['Logistic_Score'] = logistic_scores\n",
    "\n",
    "# Sort DataFrame by logistic score in descending order\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Create a new DataFrame with the required format\u001b[39;00m\n\u001b[0;32m     56\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39mranked_df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m---> 57\u001b[0m result_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloinc_num\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong_common_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomponent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimilarity\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     58\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([result_df, ranked_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Save the result DataFrame to the Excel file with the original sheet name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ander\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:849\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    848\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 849\u001b[0m iloc\u001b[38;5;241m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\ander\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1825\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1822\u001b[0m     indexer, missing \u001b[38;5;241m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[0;32m   1824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m-> 1825\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_missing(indexer, value)\n\u001b[0;32m   1826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1829\u001b[0m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ander\\Anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:2158\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   2155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_list_like_indexer(value):\n\u001b[0;32m   2156\u001b[0m         \u001b[38;5;66;03m# must have conforming columns\u001b[39;00m\n\u001b[0;32m   2157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[1;32m-> 2158\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot set a row with mismatched columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2160\u001b[0m     value \u001b[38;5;241m=\u001b[39m Series(value, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns, name\u001b[38;5;241m=\u001b[39mindexer)\n\u001b[0;32m   2162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj):\n\u001b[0;32m   2163\u001b[0m     \u001b[38;5;66;03m# We will ignore the existing dtypes instead of using\u001b[39;00m\n\u001b[0;32m   2164\u001b[0m     \u001b[38;5;66;03m#  internals.concat logic\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlsxwriter\n",
      "  Obtaining dependency information for xlsxwriter from https://files.pythonhosted.org/packages/a7/ea/53d1fe468e63e092cf16e2c18d16f50c29851242f9dd12d6a66e0d7f0d02/XlsxWriter-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "   ---------------------------------------- 0.0/159.9 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 30.7/159.9 kB ? eta -:--:--\n",
      "   --------- ----------------------------- 41.0/159.9 kB 330.3 kB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 92.2/159.9 kB 751.6 kB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 92.2/159.9 kB 751.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- 159.9/159.9 kB 686.6 kB/s eta 0:00:00\n",
      "Installing collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final ranked DataFrame has been saved to data/ranked_loinc_dataset_same_format_multi_sheets.xlsx\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
